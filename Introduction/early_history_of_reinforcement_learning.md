# 1.7 增强学习的早期历史
增强学习的早期历史中存在两条路线，都有着悠久而丰富的历史，两者一直独立发展直到现代增强学习将他们合二为一。其中一条路线研究通过试错来学习，这一想法来源于心理学中动物学习的启发。这一路线一直贯穿于人工智能研究的早期，并且带来了1980年代增强学习的复兴。另外的一条路线考虑最优控制的问题以及如何使用价值函数和动态规划来解决问题。这一路线大部分情况下不涉及学习。这两条路线都互相独立区别明显，但是第三个关注时间差分方法的路线，则相比前两者没有那么显著的区别，在本章的井字棋中使用的就是这一方法。这三条路线最终在1980年代后期融合在一起产生了现代增强学习这一领域，本书就是介绍这一领域的。

专注于试错学习的路线我们最熟悉的，也是这一简史中大量篇幅所在。在我们讨论这条线路之前，先简要讨论下最优控制路线。

“最优控制”这个词语在1950年代后期开始使用，用来描述如何设计控制器在时间维度上最小化动态系统行为的某一个指标的问题。其中的一个方法在1950年代中期由Richard Bellman和其他研究人员一起开发出来，用来扩展19世纪时Hamilton和Jacobi的理论。这一方法利用动态系统状态和价值函数（或者称作“最优回报函数”）两个概念来定义一个函数方程，现在这个方程一般被叫做Bellman方程。通过这个方程解决这类最优控制问题的各种方法就叫做动态规划（Bellman, 1957a）。Bellman（1957a）还介绍了离散概率版本的最优控制问题，这一问题被成为马尔可夫决策过程（MDPs）,Ronald Howard (1960)发明了用于MDPs的策略迭代法。而以上所说的内容都是现代增强学习的理论和各种算法的基础元素。

动态规划被广泛的认为解决通用随机最优控制的唯一可行方法。虽然这一方法受到Bellman说的“维度灾难”所困扰，“维度灾难”造成计算资源需求随着状态变量个数的增加指数级增加，动态规划依然相比其他通用方法更加有效、更具有可行性。动态规划已经被广泛的开发，包括扩展用于处理部分可观测的MDPs(Lovejoy, 1991)，各种应用（White, 1985, 1988, 1993），逼近方法（Rust, 1996），还有异步方法（Bertsekas, 1982, 1983）。目前已经有很多非常好的动态规划的方案(比如Bertsekas, 2005, 2012; Puterman, 1994; Ross, 1983; Whittle, 1982, 1983)。Bryson (1996)提供了最优控制的比较权威的历史介绍。

最优控制与动态规划的联系，或者说与学习的关系，是慢慢被发现的。我们不知道两者没有联系在一起的准确原因，但是主要原因可能是他们涉及不同学科领域，这些领域的希望达到的目标是不同的。也有可能还因为大家普遍认为动态规划是离线计算，特别依赖准确的系统模型和Bellman方程的解析解。还有就是动态规划的最简单形式是在时间上逆向的计算，这个很难让人想到如何将这个方法应用于学习过程，而学习过程必须是在时间上正向进行的。最早的动态规划的工作，比如说Bellman和Dreyfus(1959) 的，现在可以认为是遵循学习方法的路线。Witten (1977) 的工作 (下文会讨论) 可以很肯定的认为是学习和动态规划两种想法的结合。Werbos (1987)明确的提出了动态规划与学习方法的相互关系，以及与理解神经和认知模式的关系。我们已知直到Chris Watkins在1989年的工作才有了动态规划方法与在线学习的完全整合，他使用MDP的形式来解决增强学习问题的方法(Watkins, 1989)已经被广泛的应用了。在此之后这些关系就开始被大量的研究人员广泛扩展，尤其是Dimitri Bertsekas和John Tsitsiklis (1996)的工作，他们提出了“神经动态规划”，结合动态规划与神经网络。还有一个现在在用的名称就是“近似动态规划（approximate dynamic programming）”。这些方法着重于这个问题的各个不同方面，但是这些方法都可以利用增强学习来可以克服动态规划的缺点。

我们可以一定程度上认为所有最优控制相关的工作也是增强学习领域内的。增强学习方法是任何一个可以有效解决增强学习问题的手段，现在已经很明确的知道这些问题与最优控制问题都有密切的联系，特别是那些被表示为MDPs的随机最优控制问题。对应的我们就必须认为最优控制的解决方法，比如说动态规划，也是增强学习方法。由于几乎所有的传统方法都需要能够控制系统的所有信息，所以说这些方法是增强学习方法有点显得不自然。另外，很多动态规划的方法是增强和迭代的。就像学习方法一样，他们通过不断的逼近来逐渐的获得准确结果。如本书后面所展示的，这些相似点不仅仅是表面的。用来解决完备知识与不完备知识案例的理论和解决方法十分相近，以至于我们会觉得他们肯定是共同构成相同主题的一部分。


我们回到另外的一个最终引发现代增强学习领域的路线上，这一路线着重于试错学习这一想法。我们现在之简要接触这个问题的一些要点，第14章会更加详细的介绍这个主题。根据美国心理学家 R. S. Woodworth的说法，试错学习的想法可以一直追溯到1850年代Alexander Bain有关通过“摸索和试验”来学习的讨论，更加明确的提出则是英国行为学家、心理学家Conway Lloyd Morgan在1894年使用这一词语表述他对动物行为的观察(Woodworth, 1938)。可能第一个简要表达试错学习方法本质是学习的原理之一的人是Edward Thorndike：

    针对同一情景的不同反应中，其他条件相同的话，那些伴随、或者紧随动物满足感的反应，会更加与情景紧密联系，以便这一情景出现时，这些反应也更加容易出现。其他条件相同的情况下，那些伴随或者紧随动物的不适感的反应，会与情景的联系更弱，这样这一情景出现时，这些反应就更不容易出现。满意度或不适感越大，这一联系的加强或减弱就越大。(Thorndike, 1911, 244页)
    
Thorndike把以上内容称作是“有效法则”，因为其描述了强化事件对选择行为倾向的影响。Thorndike后来完善了这一法则以更好的适用动物学习研究的积累数据（比如奖励和惩罚效果的不同点），它的各种形式在学习理论家中引起了相当大的争论(比如可参考Gallistel,2005; Herrnstein, 1970; Kimble, 1961, 1967; Mazur, 1994)。尽管如此，效应定律（以某种形式）还是被广泛认为是各种动物行为背后的基础原理（例如，Hilgard和Bower，1975; Dennett，1978; Campbell，1960; Cziko，1995）。 它是Clark Hull有影响力的学习理论和B.F. Skinner的实验方法（例如Hull，1943; Skinner，1938）的基础。


在描述动物学习中使用的“强化（reinforcement）”一词是在Thorndike的效果法则中出现的表述，据我们所知，该词在该方面的第一次出现是在1927年发行的Pavlov关于条件反射的专著的英文译本中。所谓强化是指在动物受到刺激—强化物（reinforcer）后对某种行为模式的加强，该刺激与另一种刺激或反应具有一定的时态关系。一些心理学家将“强化”一词的含义从单纯的强化拓展到削弱，并将其应用于忽略某事件或某事件终止影响行为的情况中。强化过程将对行为产生持久改变，即便发起强化的强化物已经消失，所以如果一个刺激吸引了动物的注意并刺激了某种行为，但并未对动物行为产生持久性的改变，那么这种刺激物就不被认为是强化物。
    在计算机中应用试错学习的想法是在最早思考人工智能的可能性的时候就产生了的，在1948年的一篇报告中，艾伦·图灵（Alan Turing）描述了一种按照效果法则进行工作的“快乐-痛苦系统”：

         当一个配置中出现不确定的动作时，由于缺少数据，应采取一种随机的选择同时构造并采用一
         个描述性的临时的动作入口，如果因为该动作产生了痛苦刺激，那么所有临时入口被取消，如
         果因为该动作产生了快乐刺激，那么所有临时入口被转变为长期的。（Turing，1948）

许多精巧的机电机器的构造都体现了试错学习的思想。最早的这样的机器或许是Thomas Ross (1933)建造的，这个机器可以在一个简单迷宫中寻路并且记住通过交叉口的路。1951年，已经以“机械乌龟”（Walter，1950）闻名的W. Grey Walter，又搭建了一个能够实现简单学习的版本（Walter，1951）。1952年Claude Shannon展示了一只名为Theseus的老鼠，这只老鼠可以通过试错找到通过迷宫的路，而迷宫本身可以通过地板下的继电器和磁铁记录下成功通过的路径（Shannon，1951,1952）。1954年J. A. Deutsch提出了一种基于他自己行为理论（Deutsch，1953）的解决迷宫问题的机器，该机器与基于模型的强化学习方法（第8章）有一些相似性。1954年，Marvin Minsky在他的博士毕业论文中讨论了强化学习的计算模型，并描述称他构建的模拟机中包含有被他称为SNARCs（随机神经模拟增强计算器）的部分，该部分主要为了类比大脑中可修改的突出连接（第15章）。cyberneticzoo.com网站上有大量的类似这些机器的机电学习机器的信息。
    随着发展，构造机电学习机逐渐被编写数字计算机程序所取代，这些程序可以实现多种学习过程，其中一些应用了试错学习的方法。1954年Farley和Clark进行了一些数字模拟，主要针对利用试错进行学习的神经网络学习机，但之后他们的兴趣就转向了泛化和模式识别，也就是从强化学习转向了监督学习（Clark和Farley，1955），这个例子正描绘了当时很多学者对不同学习类型之间的关系难以区分的困囧局面，很多学者认为他们在从事强化学习研究，但实际上他们研究的却是监督学习。比如，神经网络领域的先驱Rosenblatt (1962)、 Widrow 和 Hoff (1960)都很明显是受到了强化学习的激励，因为他们使用类似奖励和惩罚这样的描述，但他们研究的却是适用于模式识别和知觉学习的监督学习系统。直到今天，还有一些学者或参考书会最小化或模糊不同学习类型的区别，比如，一些神经网络的参考书使用“试错”来描述网络训练的过程，这是一个可以理解的误区，因为这些网络的确使用误差信息来更新权重，但这种使用误解了试错学习的本质，也就是通过可评估的反馈来选择动作，而不依靠对于正确动作应该是什么的理解。
    这种对于学习类型区分的困境，正是导致1960和1970年代真正的试错学习变得很稀少的一个原因，尽管仍有一些值得瞩目的例外。在20世纪60年代，“强化”和“强化学习”这些词语第一次被用于工程文学，用来描述试错学习（如Waltz和Fu，1965；Mendel，1966；Fu，1970；Mendel和McClaren，1970），其中由Minsky撰写的题为《Steps Toward Artificial Intelligence》的论文产生了重大的影响，在这篇论文中讨论了一些与试错学习相关的问题，包括预测，期望以及被他称为复杂强化学习系统的基本信用分配问题（basic credit-assignment problem for complex reinforcement learning systems）：如何给实现成功过程中所做出的决定分配信用值？本书所讨论的所有方法，从某种程度上讲都是解决这个问题，Minsky的论文直到今天依然非常值得阅读。
    尽管20世纪60,70年代对于试错学习的计算和理论研究相对容易被忽视，但其中仍然有一些例外，下面几个段落我们将对这些例子进行讨论。
    这其中包括一位名叫John Andreae的新西兰研究人员的研究，Andreae在1963年开发了这一种名为STeLLA的系统，该系统可以在于环境互动的过程中进行试错学习。这个系统内部包含有关于世界的模型，并在之后又发展出了可以处理隐藏状态的“内心独白”（Andreae，1969a），之后Andreae（1977）将工作重点放在了从优秀经验中学习，但系统仍然会通过试错来学习，旨在产生新奇的结果。该系统的一个特征就是“回漏过程”，Andreae在1998年将这种过程进行了更加精妙的设计，采用了一种信用分配机制，这种机制与我们之前讨论的回溯更新操作相似。不幸的是，尽管Andreae的工作是先驱性的，但却并不著名，也未对后续的强化学习研究产生很大的影响。
    在这些研究工作中，Donald Michie的研究较Andreae有更大的影响。Donald Michie在1961年和1963年研发了一种简单的试错学习系统用来学习下井字棋，该系统叫做MENACE（Matchbox Educable Naughts and Crosses Engine可学习井字棋盒子工具）。MENACE系统为每个可能的游戏位置创建一个盒子，每个盒子中包含有一定量的彩色珠子，不同颜色代表该位置的不同可能动作。通过从对应当前位置的盒子中拿出一个珠子，MENACE可以根据珠子颜色决定下一步动作，当一句游戏结束后，通过向游戏中使用过的盒子增减珠子来强化或惩罚MENACE的决策。1968年，Michie和Chambers研发了另一种关于井字棋的名为GLEE（Game Learning
Expectimaxing Engine游戏学习专用引擎）的强化学习器，和一个名为BOXES强化学习控制器。他们使用BOXES来完成对于铰链在可移动小车上的杆子的平衡控制（倒立摆），当杆子失去平衡或小车超出可移动范围则任务失败，该控制任务是从Widrow和Smith在1964年的工作中借鉴而来，Widrow二人在解决倒立摆任务时使用了监督学习方法，过程中假设系统具备可供学习的倒立摆成功控制经验。Michie和Chambers的杆平衡任务解决方案，是早期说明强化学习可以解决知识不完备任务的最好例子之一，它影响了很多后续的的强化学习研究工作，包括一些我们自己的研究（Barto，Sutton和Anderson，1983；Sutton，1984）。后来，Michie也一直强调试错和学习是人工智能的本质（Michie，1974）。
    Widrow，Gupta和Maitra在1973年修改了Widrow和Hoff在1960年提出最小均方算法（LMS），他们通过修改LMS，使强化学习获得了从失败或成功的结果中学习的规则，而不必通过训练样本学习，他们将这种学习形式称为“选择性自举适应（selective bootstrap adaptation）”，将其形容为一种“带评论家的学习”而非“带指导者的学习”。Widrow等人分析了这种规则，并展示了强化学习算法如何通过这种新规则学习玩“二十一点”扑克游戏，Widrow的这项研究是对他对强化学习领域为数不多的一个尝试，他对于监督学习的影响要远远大于其对强化学习的影响。今天我们使用的“评论家（critic）”一词就是来源于Widrow，Gupta和Maitra的论文，1978年Buchanan, Mitchell, Smith, 和Johnson在机器学习研究中各自独立的使用了评论家这一术语（Dietterich和Buchanan在1984年也使用了该词），但是对于他们来说，评论家代表着一个专家系统，该系统可以做的不仅仅是对表现进行评估。
    对学习自动机（learning automata）的研究，对于强化学习试错方法分支有着非常直接的影响，也揭开了现代强化学习研究的篇章。这些研究所针对的都是一种非结合的，单纯的选择学习问题，该问题一般称为k臂赌博机问题（k-armed bandit），或没有k个拉杆的“独臂老虎机”问题（第2章）。学习自动机是一种为了提高在这类问题中获利概率的简单低速存储机器，最早源于俄罗斯数学家，物理学家M. L. Tsetlin和他的同事在20世纪60年代的工作中（发表于1973年，Tsetlin逝世后），自发表后学习自动机在工程领域得到了巨大的发展（Narendra和Thathachar，1974,1989），这些发展包括对于随机学习自动机的研究，该学习自动机主要用于给予奖励信号的动作概率更新。随机学习自动机出现前，一些早期的心理学研究工作为其奠定了基础，包括1950年始于William Estes并被其他学者进一步研究的学习随机理论，该理论的发展中最著名的为心理学家Robert Bush和统计学家Frederick Mosteller在1955年的研究。
    统计学习理论最初发展于心理学，而后被经济学学者采纳应用于经济学研究，并成为强化学习的一个分支。这项研究开始于1973年，最初是将Bush和Mosteller的学习理论应用于一些经典经济学模型（Cross,1973)，该研究的的一个目标就是研究比传统理想经济主题更像人类的人工主体（Arthur，1991），后来在博弈论的背景下关于统计学习的研究拓展到了强化学习领域。尽管早期经济学领域的强化学习发展独立于的人工智能研究工作，但当前强化学习和博弈论成为了两个领域学者都关心的话题，虽然本书并不讨论博弈论。Camerer（2003）讨论介绍了强化学习在经济学中的历史沿革，Nowé等人（2012）概述了从多代理角度对于强化学习的理解，这些讨论对于本书内容做出了补充。相比研究如何使用强化学习编程解决井字棋，国际跳棋以及其他有趣的游戏问题来讲，博弈论背景下的强化学习是一个非常不同的研究领域，比如Szita（2012）在这方面进行了一个关于强化学习的游戏的概述。
    John Holland（1975）概述了基于选择原则的自适应系统的一般理论，他的早期研究主要针对非结合形式的试错方法，就如同在进化方法和k臂赌博机中出现的那样。在1976年（1986年得到了进一步充实），John Holland研发了分类器系统，该系统是一种考虑了相关性和价值函数的真正的强化学习系统。这个分类器系统的一个核心部分就是用于信用分配的“桶队算法（bucket-brigade algorithm）”，该算法与我们在井字棋例子中使用的时序差分算法有密切的关系，我们将在第6章加以讨论；另一个核心部分是遗传算法（genetic algorithm），该算法是一种进化方法，主要起到发展有益部分的作用。自诞生以来，分类器系统得到了众多学者的研究与发展，现今已经成为了强化学习研究领域的一个主要分支（Urbanowicz和Moore在2009年进行了总结概述），但是遗传算法—我们并不将其本身作为强化学习系统---获得了更多的关注，就像其他进化计算方法一样（如Fogel，Owens和Walsh，1966，Koza，1992）。


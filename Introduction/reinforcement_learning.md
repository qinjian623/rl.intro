# 1.1 增强学习
增强学习是学习如何通过建立环境中不同情景与应该采取行动的关系来最大化一个数值的奖励信号。学习者不会被教导具体的行动，而是必须自己尝试不同行动来发现怎样通过一系列的行动获得最大的奖励。在那些最有趣也是最有挑战的案例中，各种行动不仅会影响即时的奖励，同时也会影响下一步的具体情景，然后进一步还会对整个后续的奖励产生影响。这两个重要的特点，不断试错和延迟奖励，是增强学习区别其他方法的最重要的特征。

增强学习，像其他类似的主题一样，比如机器学习、登山运动。既是一个问题，也包括一类可以很好应付这类问题的解决方法，同时也代表研究这个问题和问题的解决方法的领域。用一个名称来指代这三样东西只是为了方便起见，但是我们在使用这一名称的时候应该分清楚这三个不同的概念。尤其要区分好问题与解决方法两者，否则就会容易带来一系列的困扰。

我们利用动态系统理论的一些观点来形式化增强学习，更具体来说就是不完全获知的马尔可夫决策过程的最优控制。这一形式化的具体细节会在第三章揭晓，不过其中的基本理念就是直接抓住最重要的几个方面，来解决代理如何通过长时间与环境交互来达到预期目标这一问题。一个学习代理必须具有对环境状态有一定程度的感知能力，同时也必须有能力采取具体行动来影响环境的状态。代理同时还需要有一个或者多个与环境状态相关的目标。马尔可夫决策过程就用最简单的形式同时又一视同仁的包括了这三个方面：感知、行动、目标。只要是可以很好的适用于这类问题的方法我们都可以认为是一种增强学习的方法。

增强学习不同于目前在机器学习领域中被大量研究的*有监督学习*方法。有监督学习是通过由标注好的案例组成的训练集来学习的，而这些案例都是需要具有专业知识的外部指导来提供的。每一个案例都包括一个情景和系统应该针对采取的具体正确行动（也就是标注），一般来说都是判断当前情景的类别。这类学习的目标是系统最终能够推断或者归纳训练集外的情景，并且做出正确的反应。这类学习方法很重要，但是无法独立的利用交互来学习。在交互学习的问题中，获得适应代理遇到的各种情况以及需要采取的期望行为的所有案例是不可能的。在未知领域（也是学习最有用处的地方），代理必须能够通过自己积累的经验来学习。

增强学习与机器学习研究者们所说的*无监督学习*也有区别，无监督学习的目的是找到隐藏在未标注数据中的结构。监督与无监督这两个词本打算将机器学习分为两个范式，但是结果没有达到目的。由于增强学习不依赖标注了正确行为的案例，增强学习往往会被认为是一种无监督学习的方法，然而增强学习的目的并不是为了学习隐藏结构而是为了最大化奖励信号。虽然在代理的经验中学习隐藏结构在增强学习中肯定有很大作用，但是这并没有解决增强学习问题中最大化奖励信号这个问题。所以我们认为增强学习是除了监督学习和无监督学习之外的第三种范式，而且未来还有其他新的范式出现。

在增强学习中新出现的一个挑战就是探索与开发这两者的权衡，这是其他学习方法中不会遇到的。为了获得大量的奖励，增强学习代理肯定会倾向过去已经试验过的可以非常有效的产生奖励的行动。但是为了发现这些行动，它就必须去尝试之前没有使用过的行动。代理必须*开发*自己积累的经验来获得奖励，同时还需要*探索*新的行动组合以未来做出更佳的选择。其中的困境就是仅仅依靠探索或者开发两者之一都不能顺利的完成任务。代理必须尝试各种行动，*同时*慢慢的倾向选择那些开起来应该是最佳的行动。在概率性任务中，每个行动都必须尝试很多次才能够获得一个对期望奖励比较可靠的估计。探索-开发的悖论已经数学家们集中研究了几十年，至今未能解决。现在，我们简单认为权衡探索与开发两者的这个问题在监督和非监督学习中都没有出现，起码在他们最纯粹的形式上没有出现。

增强学习另外一个重要特征就是将*整个*问题明确的认为是目标导向的代理与非确定环境之间的交互。这与其他只考虑各种子问题而不具备大局观的方式形成了区别，比如，我们提到过的很多机器学习研究者都在研究监督学习但是却不考虑监督学习最后如何变得有用。其他的研究者也在推进利用通用目标规划，但是并不考虑在实时决策中规划的角色，或者在规划中模型从何而来的问题。这些研究都产生了很多有用的记过，但是他们只专注于子问题的方式是一个非常明显的限制。

增强学习反其道而行之，始于一个完整的、交互的、目标导向的代理。所有的增强学习代理都有明确的目标，可以感知周边环境的一些方面，还可以选择行动来影响周边环境。进一步，我们通常一开始就假设代理必须不断的运转，哪怕它所处的环境有多么的不确定。当增强学习涉及到规划，它就必须处理规划和实时行动选择两者的相互作用，还有环境模型如何获得、改进这个问题。当增强学习涉及到监督学习，一般都是涉及区分各种能力的关键与否这样的明确原因。为了让研究可以继续推进，重要的子问题必须要独立划分出来并且被研究，但是这些子问题必须明确扮演好自己在一个完整的、交互的、目标导向代理中的角色，哪怕目前来说整个完整的代理的一些细节还没有被完整的填充。

完整的、交互的、目标导向的代理并不总是意味着一个完整的有机体或者是机器人。有机体和机器人确实是十分典型的例子，但是代理也可以是一个更大的行为系统中的一个组件。举个简单的例子：监控机器人电池的充电水平并且发送指令到机器人的控制系统的代理。这个代理的环境就是机器人其余的部分外加机器人的外部环境。想要感受增强学习框架的通用性，就需要摆脱那些最明显的代理与其环境的例子的局限。

现代增强学习最让人兴奋的一些方面就是与其他大量的、卓有成效工程和科学学科的交互。人工智能和机器学习领域几十年来一直都在不断的更好的结合统计、优化还有其他数学学科的成果，增强学习就是这一长期趋势中的一个部分。比如，一些增强学习方法利用参数逼近方法的学习能力，可以用来处理运筹学与控制理论中经典的“维数灾难”。还有更加明显的，增强学习也可以与心理学和神经科学进行有效的交互，交互双方都可以获得大量的好处。在所有的机器学习的各种形式中，增强学习是最接近人类和动物的学习方式的，增强学习中很多的核心算法也都是最初受到生物学习系统启发而产生的。通过更加符合实验数据的有关动物学习的心理学模型，还有通过大脑奖励系统各部分的影响情况的模型，增强学习也成功反馈了结果给这些学科。本书的第14、15章就阐述了增强学习有关工程学、人工智能领域与心理学和神经科学的联系。


同时，增强学习也是人工智能回归简单通用原理这一更大趋势的一部分。1960年代晚期，很多人工智能研究者都认为不会有通用原理被发现了，智能只是大量的特殊目的技巧、过程、启发方法结合后的结果。甚至有时会有观点认为，只要我们给机器足够多的相关事实，比如一百万、甚至十亿，机器最终也会变得智能。那些依赖通用远离的方法，比如搜索、学习，都被打上“弱方法”的标签，而那些基于特定知识的方法被称作“强方法”。目前这种观点还比较常见，但是不再占统治地位。从我们视角来看，这一观点很明显是不够成熟的：对于通用原理的研究投入实在太少了，不足一得出通用原理不存在的结论。现代人工智能现在投入了很多研究去寻找学习、搜索、决策相关的通用原理，同时也尝试结合大量的领域知识。虽然现在还不知道这个问题的最终答案是什么，但是增强学习显然是倾向于认为人工智能应该适用更加简单、更加少量的通用原则。

# 更新至第8章
# A 
1.absorbing state 自循环状态
2.action value 动作价值
3.agent 代理
4.associative search 相关搜索
5.asynchronous dynamic programming 异步动态规划
# B
1.background planning 历史状态规划
2.backup diagram 回溯图
3.backward 倒推
4.backward focusing 反向聚焦
5.bandit problem 赌徒摇臂机问题
6.baseline 基线
7.batch update 批量更新
8.Bayesian method 贝叶斯方法
9.Bellman equation 贝尔曼方程
10.Bellman optimality equation 贝尔曼最优方程
11.bias 偏差
12.blackjack “二十一点”扑克牌游戏
13.bootstrap 自举
14.branching factor 分支因数
# C
1.certainty-equivalence estimate 确定等价估计
2.confidence level 置信水平
3.conjugate priors 共轭先验
4.contextual bandits 情境赌徒摇臂机问题
5.continuing task 连续任务
6.convergence 收敛性
# D
1.decision-time planning 即时状态规划
2.decision tree 决策树
3.discount rate 衰减率
4.distribution model 分布模型
5.dynamic programming 动态规划
# E
1.eligibility trace 资格迹
2.episode 回合，片段
3.episodic task 回合任务
4.equiprobable random policy 等概率随机策略
5.error 误差
6.every-visit MC method 全访问蒙特卡洛（MC）方法
7.expectation 数学期望
8.expected update 期望更新
9.exploitation 利用
10.exploration 探索
11.exploring starts 探索性起始
12.exponential recency-weighted average 指数近因加权平均
# F
1.feature vector 特征向量
2.finite MDP 有限MDP
3.first-visit MC method 首访问蒙特卡洛（MC）方法
4.forward focusing 前向聚焦
5.function approximation 函数近似（函数逼近）
# G
1.generalization 归纳/泛化
2.generalized policy iteration（GPI） 一般策略迭代
3.global optimum 全局最优
4.goal 目标
5.gradient 梯度
6.gradient bandit algorithm 
7.greedy action 贪婪动作
# H
1.heuristic search 启发式搜索
# I
1.immediate reward 即时奖励
2.importance sampling 重要性抽样
3.incremental 增量式
4.interaction 交互
5.iterative policy evaluation 迭代策略评估
6.iterative solution method 迭代求解方法
# J
# K
1.K-armed bandit problem k臂赌徒摇臂机问题
# L
1.leaning 学习
.local optimum 局部最优
# M
1.Markov decision processes(MDP) 马尔科夫决策过程
2.maximum-likelihood estimate 最大似然估计
3.Mean Squared Value Error 均方误差
4.Monte Carlo method 蒙特卡洛方法
5.Monte Carlo Tree Search(MCTS) 蒙特卡洛树搜索
# N
1.natural logarithm 自然对数
2.neural network 神经网络
3.nonassociative 非相关
4.nonstationary 非稳定
5.normal distribution 正态分布
6.normal Gaussian distribution 正态分布
# O
1.off-policy 策略离线
2.on-policy 策略在线
3.one-step-ahead search 一步前向搜索
4.optimal policy 最优策略
5.optimal value function 最优价值函数
6.optimistic initial values 乐观初始值方法
# P
1.parameter study 参数研究图
2.partial derivative 偏导数
3.pattern classification 模式分类
4.penalty 惩罚，罚值
5.planning 规划
6.policy 策略
7.policy-gradient method 梯度下降法
8.policy evaluation 策略评估
9.policy improvement 策略改进
10.policy iteration 策略迭代
11.polynomial 多项式
12.prediction problem 预测问题
13.prior knowledge 先验知识
14.prioritized sweeping 优先（级）更新
15.probability distribution 概率分布
16.propagation 传播
# Q
# R
1.real-time 实时
2.reinforcement learning 强化学习
3.return 回报
4.reward 奖励
5.rollout algorithm 推展算法
6.root node 根节点
# S
1.sample-average method 样本平均法
2.sample complexity 抽样复杂度
3.sample model 样本模型
4.sample update 样本更新
5.scalar 标量
6.search tree 搜索树
7.self-consistency 自洽
8.semi-gradient method 半梯度方法
9.sequential decision making 序贯决策
10.simulated experience 模拟经验
11.situation 情境
12.slot machine 老虎机
13.soft-max distribution 柔性最大值分布
14.square-root 平方根
15.state aggregation 状态聚合
16.state space 状态空间
17.state-transition probabilities 状态转移概率
18.state-value 状态价值
19.stationarity 稳定性
20.step-size parameter 步长参数
21.stochastic approximation theory 随机近似理论
22.stochastic gradient descent (SGD)随机梯度下降
23.supervised learning 监督学习
24.sweep 全局更新/遍历？
25.system identification 系统辨识
# T
1.tabular method 查表方法
2.temporal-difference(TD) 时序差分
3.terminal state 终止状态
4.the law of large numbers 大数定律
5.time step 时间步
6.trajectory sampling 踪迹（/轨迹）采样
7.tree-backup algorithm 回溯树算法
8.trial-and-error learning 试错学习
# U
1.uncertain/uncertainty 不确定性
2.unsupervised learning 非监督学习
3.upper confidence bound (UCB) 上限置信区间算法
# V
1.value function 价值函数
2.value iteration 价值迭代
3.variance 方差
# W
1.weight 权重
# X
# Y
# Z
# 其他
1.ε-greedy method ε贪婪方法
